{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_reviews.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "O0LOTm3G2OsE"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakash-tech89/Data-Science/blob/master/Python/Hackathon/Sentiment_Analysis_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDhioWsRD326",
        "colab_type": "text"
      },
      "source": [
        "# Data load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtZyYDP3rVDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIq0gi9PqvjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df=pd.read_csv(\"/content/review_train.csv\",sep=\",\")\n",
        "test_df=pd.read_csv(\"/content/review_test.csv\",sep=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEmLCl3NMOGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df[\"source\"] = \"train\"\n",
        "test_df[\"source\"] = \"test\"\n",
        "df = pd.concat([train_df,test_df])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f6bVPFoDUY0",
        "colab_type": "text"
      },
      "source": [
        "# Using NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOtHNu4O2cPT",
        "colab_type": "text"
      },
      "source": [
        "## Loading Necessary Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3jz911CqF5t",
        "colab_type": "code",
        "outputId": "fe3d8aff-aa8f-4df0-d842-96a8036ffb6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "!pip install textsearch\n",
        "!pip install contractions\n",
        "!pip install tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (0.0.17)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch) (1.1.1)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch) (1.4.0)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.21)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XgPbs5Tqovs",
        "colab_type": "code",
        "outputId": "be5d6590-95aa-4273-b33a-a71095058a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "!python -m spacy validate"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r⠙ Loading compatibility table...\r\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
            "\u001b[1m\n",
            "====================== Installed models (spaCy v2.1.8) ======================\u001b[0m\n",
            "\u001b[38;5;4mℹ spaCy installation: /usr/local/lib/python3.6/dist-packages/spacy\u001b[0m\n",
            "\n",
            "TYPE      NAME             MODEL            VERSION                            \n",
            "package   en-core-web-sm   en_core_web_sm   \u001b[38;5;2m2.1.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "link      en               en_core_web_sm   \u001b[38;5;2m2.1.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eIb2r52qsEO",
        "colab_type": "code",
        "outputId": "0523acda-3d19-4c07-e449-f15bfb11ee03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "!python -m spacy download en\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efoogbsZBfbF",
        "colab_type": "code",
        "outputId": "a8eff29e-1379-4e48-de5b-8a804e8fb751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nltk.download('all')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0LOTm3G2OsE",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0nvMjGpq6eL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import contractions\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def tokenize_sent(text):\n",
        "    return (nltk.sent_tokenize(text))\n",
        "\n",
        "def to_lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "def lemmatized_text(tokens):\n",
        "    return wnl.lemmatize(tokens)\n",
        "def tokenize_word(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "def pos_tag(text):\n",
        "    return nltk.pos_tag(text)\n",
        "def pos_tag_wordnet(tagged_tokens):\n",
        "    tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
        "    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN))\n",
        "                            for word, tag in tagged_tokens]\n",
        "    return new_tagged_tokens\n",
        "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
        "    if not stopwords:\n",
        "        stopwords = nltk.corpus.stopwords.words('english')\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    \n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "    \n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "def normalize_document(doc):\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return np.array(doc)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucgkollq6oC0",
        "colab_type": "code",
        "outputId": "d34cb2f6-3ff5-40a2-ec39-4ca666b4fb94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "df[\"pre-processed\"]=df.apply(lambda x : (tokenize_sent(str(x[\"Text\"]))),axis=1)\n",
        "df[\"pre-processed\"].head(7)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [I got a free sample of these once, and now--w...\n",
              "1    [I used to get this Tea when I lived in Washin...\n",
              "2    [This is my all time favorite 'grab and go' sn...\n",
              "3    [This flavor is very good and unexpected., The...\n",
              "4    [thrilled to have this assortment as i got the...\n",
              "5    [I love the flavor of this coffee., The only d...\n",
              "6    [Like the last review, I have a mixture of dog...\n",
              "Name: pre-processed, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkuCWlOn9pk5",
        "colab_type": "code",
        "outputId": "97d870b8-cea6-4ea5-9d82-abf3ed4db027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "df[\"pre-processed\"]=df.apply(lambda x : remove_special_characters(str(x[\"pre-processed\"]),remove_digits=False) ,axis=1)\n",
        "df[\"pre-processed\"].head(7)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    I got a free sample of these once and nowwere ...\n",
              "1    I used to get this Tea when I lived in Washing...\n",
              "2    This is my all time favorite grab and go snack...\n",
              "3    This flavor is very good and unexpected The fl...\n",
              "4    thrilled to have this assortment as i got the ...\n",
              "5    I love the flavor of this coffee The only draw...\n",
              "6    Like the last review I have a mixture of dogs ...\n",
              "Name: pre-processed, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iFRpcBYAc_h",
        "colab_type": "code",
        "outputId": "43e61157-c8a0-4a03-f0f6-d0f50bdf1feb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "df[\"pre-processed\"]=df.apply(lambda x : expand_contractions(x[\"pre-processed\"]),axis=1)\n",
        "df[\"pre-processed\"].head(7)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    I got a free sample of these once and nowwere ...\n",
              "1    I used to get this Tea when I lived in Washing...\n",
              "2    This is my all time favorite grab and go snack...\n",
              "3    This flavor is very good and unexpected The fl...\n",
              "4    thrilled to have this assortment as i got the ...\n",
              "5    I love the flavor of this coffee The only draw...\n",
              "6    Like the last review I have a mixture of dogs ...\n",
              "Name: pre-processed, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFNYRsI2pmU4",
        "colab_type": "code",
        "outputId": "ec4dcd1f-236a-4f1a-ff4c-79c21767d470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "\n",
        "df[\"pre-processed\"]=df.apply(lambda x : to_lower(x[\"pre-processed\"]),axis=1)\n",
        "df[\"pre-processed\"].head(7)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    i got a free sample of these once and nowwere ...\n",
              "1    i used to get this tea when i lived in washing...\n",
              "2    this is my all time favorite grab and go snack...\n",
              "3    this flavor is very good and unexpected the fl...\n",
              "4    thrilled to have this assortment as i got the ...\n",
              "5    i love the flavor of this coffee the only draw...\n",
              "6    like the last review i have a mixture of dogs ...\n",
              "Name: pre-processed, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdQGYY2nCSmC",
        "colab_type": "code",
        "outputId": "5991e9b9-7d6f-4d41-e0d0-3931df5b11dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "df[\"pre-processed\"]=df.apply(lambda x : remove_stopwords(str(x[\"pre-processed\"])),axis=1)\n",
        "df[\"pre-processed\"].head(7)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    got free sample nowwere subscribe save program...\n",
              "1    used get tea lived washington state missed nie...\n",
              "2    time favorite grab go snack bar leave sugar ru...\n",
              "3    flavor good unexpected flavor cheese garlic go...\n",
              "4    thrilled assortment got chance choose another ...\n",
              "5    love flavor coffee drawback set largest cup wa...\n",
              "6    like last review mixture dogs 2 chihuahuas eat...\n",
              "Name: pre-processed, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU9x7KZPAhV6",
        "colab_type": "code",
        "outputId": "c6a6e4dc-5d58-40a4-9a14-70044f0a6873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "df[\"tokenized\"]=df.apply(lambda x : tokenize_word(x[\"pre-processed\"]),axis=1)\n",
        "df[\"tokenized\"].head(7)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [got, free, sample, nowwere, subscribe, save, ...\n",
              "1    [used, get, tea, lived, washington, state, mis...\n",
              "2    [time, favorite, grab, go, snack, bar, leave, ...\n",
              "3    [flavor, good, unexpected, flavor, cheese, gar...\n",
              "4    [thrilled, assortment, got, chance, choose, an...\n",
              "5    [love, flavor, coffee, drawback, set, largest,...\n",
              "6    [like, last, review, mixture, dogs, 2, chihuah...\n",
              "Name: tokenized, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKh-OhloBF6X",
        "colab_type": "code",
        "outputId": "384a0ec5-2198-4d22-f5a7-6078d6f06fac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "df[\"pre-processed-tag\"]=df.apply(lambda x : pos_tag(x[\"tokenized\"]),axis=1)\n",
        "df[\"pre-processed-tag\"].head(7)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [(got, VBD), (free, JJ), (sample, NN), (nowwer...\n",
              "1    [(used, VBN), (get, VB), (tea, JJ), (lived, JJ...\n",
              "2    [(time, NN), (favorite, JJ), (grab, NN), (go, ...\n",
              "3    [(flavor, NN), (good, JJ), (unexpected, JJ), (...\n",
              "4    [(thrilled, JJ), (assortment, NN), (got, VBD),...\n",
              "5    [(love, VB), (flavor, NN), (coffee, NN), (draw...\n",
              "6    [(like, IN), (last, JJ), (review, NN), (mixtur...\n",
              "Name: pre-processed-tag, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLLWGs68BNcY",
        "colab_type": "code",
        "outputId": "877c378a-80af-41d0-81ed-f1e1e4ba449a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "df[\"pre-processed-wordnet\"]=df.apply(lambda x : pos_tag_wordnet(x[\"pre-processed-tag\"]),axis=1)\n",
        "df[\"pre-processed-wordnet\"].head(7)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [(got, v), (free, a), (sample, n), (nowwere, r...\n",
              "1    [(used, v), (get, v), (tea, a), (lived, a), (w...\n",
              "2    [(time, n), (favorite, a), (grab, n), (go, v),...\n",
              "3    [(flavor, n), (good, a), (unexpected, a), (fla...\n",
              "4    [(thrilled, a), (assortment, n), (got, v), (ch...\n",
              "5    [(love, v), (flavor, n), (coffee, n), (drawbac...\n",
              "6    [(like, n), (last, a), (review, n), (mixture, ...\n",
              "Name: pre-processed-wordnet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myu4ZSww2DEa",
        "colab_type": "text"
      },
      "source": [
        "## Basic and Deep neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmJw6kLjNs8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_final = df[df.source==\"train\"]\n",
        "test_final = df[df.source==\"test\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36m92EOBYChF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build train and test datasets\n",
        "reviews = train_final['Text'].values\n",
        "sentiments = train_final['Sentiment'].values\n",
        "\n",
        "train_reviews = reviews[:7400]\n",
        "train_sentiments = sentiments[:7400]\n",
        "\n",
        "test_reviews = reviews[7400:]\n",
        "test_sentiments = sentiments[7400:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqKAPhKtZlN9",
        "colab_type": "code",
        "outputId": "4362a3c3-d89b-42dd-cf77-2f5fc14144cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_reviews.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7425,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe0W9AhY33Zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import re\n",
        "import tqdm\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "def strip_html_tags(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  [s.extract() for s in soup(['iframe', 'script'])]\n",
        "  stripped_text = soup.get_text()\n",
        "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "  return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text\n",
        "\n",
        "def pre_process_corpus(docs):\n",
        "  norm_docs = []\n",
        "  for doc in tqdm.tqdm(docs):\n",
        "    doc = strip_html_tags(doc)\n",
        "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    doc = doc.lower()\n",
        "    doc = remove_accented_chars(doc)\n",
        "    doc = contractions.fix(doc)\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
        "    doc = re.sub(' +', ' ', doc)\n",
        "    doc = doc.strip()  \n",
        "    norm_docs.append(doc)\n",
        "  \n",
        "  return norm_docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiQjnYEMquiU",
        "colab_type": "code",
        "outputId": "9aa35312-e10d-4938-f026-52935b3b8b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "norm_train_reviews = pre_process_corpus(train_reviews)\n",
        "norm_test_reviews = pre_process_corpus(test_reviews)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7400/7400 [00:01<00:00, 4528.84it/s]\n",
            "100%|██████████| 7425/7425 [00:01<00:00, 4489.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.24 s, sys: 36.2 ms, total: 3.28 s\n",
            "Wall time: 3.3 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b4hBOdj2tGj",
        "colab_type": "code",
        "outputId": "d182fe4a-6775-41fc-dadc-25d348e5c1fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# build BOW features on train reviews\n",
        "cv = CountVectorizer(binary=False,min_df=5, max_df=1.0,  ngram_range=(1,2))\n",
        "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
        "\n",
        "\n",
        "# build TFIDF features on train reviews\n",
        "tv = TfidfVectorizer(use_idf=True,min_df=5, max_df=1.0,  ngram_range=(1,2),\n",
        "                     sublinear_tf=True)\n",
        "tv_train_features = tv.fit_transform(norm_train_reviews)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.22 s, sys: 74.9 ms, total: 3.29 s\n",
            "Wall time: 3.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw2U_p4vODaO",
        "colab_type": "code",
        "outputId": "285e1b83-e010-4a82-b461-a1b057da34ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# transform test reviews into features\n",
        "cv_test_features = cv.transform(norm_test_reviews)\n",
        "tv_test_features = tv.transform(norm_test_reviews)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.64 s, sys: 4.94 ms, total: 1.65 s\n",
            "Wall time: 1.65 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU_kaihoNn4A",
        "colab_type": "code",
        "outputId": "a835ad46-aa1c-47d9-c5cf-a0ab27b507c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
        "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOW model:> Train features shape: (7400, 19814)  Test features shape: (7425, 19814)\n",
            "TFIDF model:> Train features shape: (7400, 19814)  Test features shape: (7425, 19814)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_5200VQVe4l",
        "colab_type": "text"
      },
      "source": [
        "### Model Training, Prediction and Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlfaofXbKc8z",
        "colab_type": "code",
        "outputId": "2fbb7acf-7607-4542-dc4c-98d886523552",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Logistic Regression model on BOW features\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# instantiate model\n",
        "lr = LogisticRegression(penalty='l2', max_iter=500, C=1, solver='lbfgs', random_state=42)\n",
        "\n",
        "# train model\n",
        "lr.fit(cv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "lr_bow_predictions = lr.predict(cv_test_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.3 s, sys: 870 ms, total: 2.17 s\n",
            "Wall time: 1.12 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHWnds9bVzjx",
        "colab_type": "code",
        "outputId": "4c889bd8-b962-44e6-e8cf-df840af33646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, lr_bow_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, lr_bow_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.61      0.68      1201\n",
            "           1       0.93      0.97      0.95      6224\n",
            "\n",
            "    accuracy                           0.91      7425\n",
            "   macro avg       0.86      0.79      0.82      7425\n",
            "weighted avg       0.90      0.91      0.90      7425\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>729</td>\n",
              "      <td>472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>199</td>\n",
              "      <td>6025</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative       729       472\n",
              "positive       199      6025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTy_KHh6WDbd",
        "colab_type": "code",
        "outputId": "fde83bdd-1832-4004-8507-6347cb4aaf4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, lr_bow_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, lr_bow_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.61      0.68      1201\n",
            "           1       0.93      0.97      0.95      6224\n",
            "\n",
            "    accuracy                           0.91      7425\n",
            "   macro avg       0.86      0.79      0.82      7425\n",
            "weighted avg       0.90      0.91      0.90      7425\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>729</td>\n",
              "      <td>472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>199</td>\n",
              "      <td>6025</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative       729       472\n",
              "positive       199      6025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BapR-ZUaaCzQ",
        "colab_type": "code",
        "outputId": "64060473-4390-4482-e691-de14ef157acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Random Forest model on BOW features\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# instantiate model\n",
        "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
        "\n",
        "# train model\n",
        "rf.fit(cv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "rf_bow_predictions = rf.predict(cv_test_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 8.42 s, sys: 26 ms, total: 8.44 s\n",
            "Wall time: 4.37 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdC3uX8oajdP",
        "colab_type": "code",
        "outputId": "4e6f7ab8-0c75-437f-9d75-750889b0bf18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, rf_bow_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, rf_bow_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.15      0.26      1201\n",
            "           1       0.86      1.00      0.92      6224\n",
            "\n",
            "    accuracy                           0.86      7425\n",
            "   macro avg       0.91      0.58      0.59      7425\n",
            "weighted avg       0.88      0.86      0.82      7425\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>183</td>\n",
              "      <td>1018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>7</td>\n",
              "      <td>6217</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative       183      1018\n",
              "positive         7      6217"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A3n1R7RaqAC",
        "colab_type": "code",
        "outputId": "1833e1a6-ec02-47b9-ba5b-f5d86e7ba10d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Random Forest model on TF-IDF features\n",
        "\n",
        "# train model\n",
        "rf.fit(tv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "rf_tfidf_predictions = rf.predict(tv_test_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10.3 s, sys: 23.4 ms, total: 10.3 s\n",
            "Wall time: 5.28 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2DzIrsWauKm",
        "colab_type": "code",
        "outputId": "46c2e46b-791e-45d2-ea9c-5081f7ebd219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, rf_tfidf_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, rf_tfidf_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.15      0.25      1201\n",
            "           1       0.86      1.00      0.92      6224\n",
            "\n",
            "    accuracy                           0.86      7425\n",
            "   macro avg       0.91      0.57      0.59      7425\n",
            "weighted avg       0.88      0.86      0.81      7425\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>175</td>\n",
              "      <td>1026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>6</td>\n",
              "      <td>6218</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative       175      1026\n",
              "positive         6      6218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqfTaswFaxZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Activation, Dense\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r0YS8wFa0MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "le = LabelEncoder()\n",
        "# tokenize train reviews & encode train labels\n",
        "tokenized_train = [nltk.word_tokenize(text)\n",
        "                       for text in norm_train_reviews]\n",
        "y_train = le.fit_transform(train_sentiments)\n",
        "# tokenize test reviews & encode test labels\n",
        "tokenized_test = [nltk.word_tokenize(text)\n",
        "                       for text in norm_test_reviews]\n",
        "y_test = le.fit_transform(test_sentiments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRp2mUk1a3Cz",
        "colab_type": "code",
        "outputId": "96b0b685-7b81-43f0-974f-49643d78b72d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# print class label encoding map and encoded labels\n",
        "print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
        "print('Sample test label transformation:\\n'+'-'*35,\n",
        "      '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_test[:3])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment class label map: {0: 0, 1: 1}\n",
            "Sample test label transformation:\n",
            "----------------------------------- \n",
            "Actual Labels: [1 1 0] \n",
            "Encoded Labels: [1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu-qsT4Gbofs",
        "colab_type": "text"
      },
      "source": [
        "### Feature Engineering with word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ee206tAbG4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm0467ICbLO7",
        "colab_type": "code",
        "outputId": "1663a62d-ab7f-45ab-9abe-9f00a60a2220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "%%time\n",
        "# build word2vec model\n",
        "w2v_num_features = 300\n",
        "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,\n",
        "                                   min_count=10, workers=4, iter=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-15 09:15:58,283 : INFO : collecting all words and their counts\n",
            "2019-09-15 09:15:58,285 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-09-15 09:15:58,394 : INFO : collected 24018 word types from a corpus of 576790 raw words and 7400 sentences\n",
            "2019-09-15 09:15:58,395 : INFO : Loading a fresh vocabulary\n",
            "2019-09-15 09:15:58,421 : INFO : effective_min_count=10 retains 3380 unique words (14% of original 24018, drops 20638)\n",
            "2019-09-15 09:15:58,422 : INFO : effective_min_count=10 leaves 537209 word corpus (93% of original 576790, drops 39581)\n",
            "2019-09-15 09:15:58,439 : INFO : deleting the raw counts dictionary of 24018 items\n",
            "2019-09-15 09:15:58,441 : INFO : sample=0.001 downsamples 59 most-common words\n",
            "2019-09-15 09:15:58,442 : INFO : downsampling leaves estimated 378705 word corpus (70.5% of prior 537209)\n",
            "2019-09-15 09:15:58,454 : INFO : estimated required memory for 3380 words and 300 dimensions: 9802000 bytes\n",
            "2019-09-15 09:15:58,455 : INFO : resetting layer weights\n",
            "2019-09-15 09:15:58,500 : INFO : training model with 4 workers on 3380 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=150\n",
            "2019-09-15 09:15:59,546 : INFO : EPOCH 1 - PROGRESS: at 33.47% examples, 118652 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 09:16:00,609 : INFO : EPOCH 1 - PROGRESS: at 70.43% examples, 126798 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 09:16:01,260 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-15 09:16:01,289 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-15 09:16:01,321 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-15 09:16:01,335 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-15 09:16:01,336 : INFO : EPOCH - 1 : training on 576790 raw words (378724 effective words) took 2.8s, 133850 effective words/s\n",
            "2019-09-15 09:16:02,357 : INFO : EPOCH 2 - PROGRESS: at 33.28% examples, 121902 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 09:16:03,415 : INFO : EPOCH 2 - PROGRESS: at 70.43% examples, 128751 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 09:16:04,093 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-15 09:16:04,105 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-15 09:16:04,129 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-15 09:16:04,163 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-15 09:16:04,164 : INFO : EPOCH - 2 : training on 576790 raw words (378826 effective words) took 2.8s, 134296 effective words/s\n",
            "2019-09-15 09:16:05,172 : INFO : EPOCH 3 - PROGRESS: at 33.47% examples, 123343 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 09:16:06,194 : INFO : EPOCH 3 - PROGRESS: at 67.11% examples, 125358 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 09:16:06,972 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-15 09:16:06,987 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-15 09:16:06,989 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-15 09:16:07,004 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-15 09:16:07,006 : INFO : EPOCH - 3 : training on 576790 raw words (378800 effective words) took 2.8s, 133603 effective words/s\n",
            "2019-09-15 09:16:08,121 : INFO : EPOCH 4 - PROGRESS: at 36.76% examples, 124257 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 09:16:09,131 : INFO : EPOCH 4 - PROGRESS: at 72.27% examples, 128980 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 09:16:09,744 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-15 09:16:09,772 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-15 09:16:09,811 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-15 09:16:09,829 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-15 09:16:09,830 : INFO : EPOCH - 4 : training on 576790 raw words (379021 effective words) took 2.8s, 134459 effective words/s\n",
            "2019-09-15 09:16:10,854 : INFO : EPOCH 5 - PROGRESS: at 33.47% examples, 120954 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 09:16:11,917 : INFO : EPOCH 5 - PROGRESS: at 70.43% examples, 127993 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 09:16:12,580 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-15 09:16:12,606 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-15 09:16:12,634 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-15 09:16:12,664 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-15 09:16:12,665 : INFO : EPOCH - 5 : training on 576790 raw words (378391 effective words) took 2.8s, 133705 effective words/s\n",
            "2019-09-15 09:16:12,666 : INFO : training on a 2883950 raw words (1893762 effective words) took 14.2s, 133693 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 28 s, sys: 55.8 ms, total: 28 s\n",
            "Wall time: 14.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYyJl3QQb_ts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    \n",
        "    def average_word_vectors(words, model, vocabulary, num_features):\n",
        "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "        nwords = 0.\n",
        "        \n",
        "        for word in words:\n",
        "            if word in vocabulary: \n",
        "                nwords = nwords + 1.\n",
        "                feature_vector = np.add(feature_vector, model.wv[word])\n",
        "        if nwords:\n",
        "            feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "        return feature_vector\n",
        "\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYRBt2azdkbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate averaged word vector features from word2vec model\n",
        "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
        "                                                     num_features=w2v_num_features)\n",
        "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
        "                                                    num_features=w2v_num_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "837vL1VJd824",
        "colab_type": "code",
        "outputId": "e45be697-0ad7-4657-f5bc-8c75707746c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec model:> Train features shape: (7400, 300)  Test features shape: (7425, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk3kg-Y-eTNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_deepnn_architecture(num_input_features):\n",
        "    dnn_model = Sequential()\n",
        "    dnn_model.add(Dense(512, input_shape=(num_input_features,)))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(1))\n",
        "    dnn_model.add(Activation('sigmoid'))\n",
        "\n",
        "    dnn_model.compile(loss='binary_crossentropy', optimizer='adam',                 \n",
        "                      metrics=['accuracy'])\n",
        "    return dnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsT5Nl2dezrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBpX62r2e8y6",
        "colab_type": "code",
        "outputId": "c8dc38f5-b055-480b-b09d-2290de730269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "w2v_dnn.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 512)               154112    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 351,489\n",
            "Trainable params: 351,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fsUMAkae_da",
        "colab_type": "code",
        "outputId": "f2d06a19-a6f0-44ec-8889-29cffe078387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "batch_size = 100\n",
        "w2v_dnn.fit(avg_wv_train_features, y_train, epochs=10, batch_size=batch_size, \n",
        "            shuffle=True, validation_split=0.1, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6660 samples, validate on 740 samples\n",
            "Epoch 1/10\n",
            "6660/6660 [==============================] - 1s 122us/sample - loss: 0.3713 - acc: 0.8449 - val_loss: 0.3648 - val_acc: 0.8324\n",
            "Epoch 2/10\n",
            "6660/6660 [==============================] - 1s 93us/sample - loss: 0.3268 - acc: 0.8646 - val_loss: 0.3548 - val_acc: 0.8311\n",
            "Epoch 3/10\n",
            "6660/6660 [==============================] - 1s 92us/sample - loss: 0.3215 - acc: 0.8671 - val_loss: 0.3537 - val_acc: 0.8392\n",
            "Epoch 4/10\n",
            "6660/6660 [==============================] - 1s 91us/sample - loss: 0.3156 - acc: 0.8701 - val_loss: 0.3515 - val_acc: 0.8405\n",
            "Epoch 5/10\n",
            "6660/6660 [==============================] - 1s 91us/sample - loss: 0.3138 - acc: 0.8712 - val_loss: 0.3509 - val_acc: 0.8500\n",
            "Epoch 6/10\n",
            "6660/6660 [==============================] - 1s 91us/sample - loss: 0.3086 - acc: 0.8727 - val_loss: 0.3466 - val_acc: 0.8419\n",
            "Epoch 7/10\n",
            "6660/6660 [==============================] - 1s 93us/sample - loss: 0.3072 - acc: 0.8707 - val_loss: 0.3515 - val_acc: 0.8527\n",
            "Epoch 8/10\n",
            "6660/6660 [==============================] - 1s 90us/sample - loss: 0.3071 - acc: 0.8731 - val_loss: 0.3463 - val_acc: 0.8365\n",
            "Epoch 9/10\n",
            "6660/6660 [==============================] - 1s 93us/sample - loss: 0.3049 - acc: 0.8757 - val_loss: 0.3539 - val_acc: 0.8486\n",
            "Epoch 10/10\n",
            "6660/6660 [==============================] - 1s 93us/sample - loss: 0.3036 - acc: 0.8733 - val_loss: 0.3544 - val_acc: 0.8405\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff411c34748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGgiP0hhfDkP",
        "colab_type": "code",
        "outputId": "9a0e016d-6ffc-42c5-d0bb-5129ad5896ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\n",
        "predictions = le.inverse_transform(y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:273: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzKM6IsEfHOL",
        "colab_type": "code",
        "outputId": "b036f00d-8655-4a91-9a04-507d3b1e73ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, predictions), index=labels, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.22      0.33      1201\n",
            "           1       0.87      0.97      0.92      6224\n",
            "\n",
            "    accuracy                           0.85      7425\n",
            "   macro avg       0.75      0.60      0.62      7425\n",
            "weighted avg       0.83      0.85      0.82      7425\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>265</td>\n",
              "      <td>936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>156</td>\n",
              "      <td>6068</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative       265       936\n",
              "positive       156      6068"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1D42wu5fJfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlJfC6GNDfYu",
        "colab_type": "text"
      },
      "source": [
        "# Using Lexican Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSl_NOOoDSDR",
        "colab_type": "code",
        "outputId": "8a8084aa-3e8a-4a55-91b5-4fdfdd278a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "!pip install textblob\n",
        "!pip install textsearch\n",
        "!pip install contractions\n",
        "!pip install afinn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.12.0)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (0.0.17)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch) (1.4.0)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch) (1.1.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.21)\n",
            "Collecting afinn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/e5/ffbb7ee3cca21ac6d310ac01944fb163c20030b45bda25421d725d8a859a/afinn-0.1.tar.gz (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-cp36-none-any.whl size=53453 sha256=422e4c1230466fbc6fe891284a2656f8aea5db0eaa62d08495f92db46419a9aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/1c/de/428301f3333ca509dcf20ff358690eb23a1388fbcbbde008b2\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLTISvrHENr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import textblob\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "np.set_printoptions(precision=2, linewidth=80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-k9qwiZHQVm",
        "colab_type": "text"
      },
      "source": [
        "## Using TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzowmtnUDSAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews = np.array(train_df['Text'])\n",
        "sentiments = np.array(train_df['Sentiment'])\n",
        "\n",
        "# extract data for model evaluation\n",
        "test_reviews = reviews[7400:]\n",
        "test_sentiments = sentiments[7400:]\n",
        "sample_review_ids = [4626, 3533, 4]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p6YPXXvEZ-z",
        "colab_type": "text"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chGD80qcDR9N",
        "colab_type": "code",
        "outputId": "4446c581-b5d6-42d6-e853-1b3b9e1fcd36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
        "    print('REVIEW:', review)\n",
        "    print('Actual Sentiment:', sentiment)\n",
        "    print('Predicted Sentiment polarity:', textblob.TextBlob(review).sentiment.polarity)\n",
        "    print('-'*60)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REVIEW: I am a firm believer and user of Maple Syrup over Corn Syrup or white sugar.  It is, simply, a healthier sugar - if sugar can be healthy.<br /><br />I'm sure the syrup will meet all of my taste expectations.<br /><br />The syrup was securely packaged and arrived sooner than I expected.  No complaints.\n",
            "Actual Sentiment: 1\n",
            "Predicted Sentiment polarity: 0.09999999999999999\n",
            "------------------------------------------------------------\n",
            "REVIEW: I really wanted to find a rice cereal that had more nutritional value than just plain white rice and I am so happy I found this product. I started my baby on this at 5 months and he loves it. I mix it with Breast Milk and once he starts veggies and fruit I will add that in as well. This does not constipate him either and I love that it is Organic and enriched with Probiotcs and DHA.<br />If you are looking for a great first food for your baby with quality ingredients this is it and you can't beat the price for organic.\n",
            "Actual Sentiment: 1\n",
            "Predicted Sentiment polarity: 0.3544642857142857\n",
            "------------------------------------------------------------\n",
            "REVIEW: We drink a lot of green tea and TAZO is one of our favorites. It's a good deal when I buy it on amazon prime. 6 boxes of 20 for $4\n",
            "Actual Sentiment: 1\n",
            "Predicted Sentiment polarity: 0.24999999999999997\n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPewzE90DR6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_polarity = [textblob.TextBlob(review).sentiment.polarity for review in test_reviews]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55JfyJw7DR3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_sentiments = [1 if score >= 0.1 else 0 for score in sentiment_polarity]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BxyUVm9GnI5",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8_tc6SaDR1L",
        "colab_type": "code",
        "outputId": "a6a951ac-66eb-4144-fac0-c1c7b92d56fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, predicted_sentiments))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, predicted_sentiments), index=labels, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.65      0.52      1201\n",
            "           1       0.93      0.84      0.88      6224\n",
            "\n",
            "    accuracy                           0.81      7425\n",
            "   macro avg       0.68      0.74      0.70      7425\n",
            "weighted avg       0.85      0.81      0.82      7425\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>780</td>\n",
              "      <td>421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>998</td>\n",
              "      <td>5226</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative       780       421\n",
              "positive       998      5226"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL2e2tcEHrhx",
        "colab_type": "text"
      },
      "source": [
        "## Using Afinn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecjcAuSqDRyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from afinn import Afinn\n",
        "\n",
        "afn = Afinn(emoticons=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rFCF_inH7L4",
        "colab_type": "text"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXTOT5UwDRwY",
        "colab_type": "code",
        "outputId": "25df98ce-0026-4445-a84b-fe2e594224f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
        "    print('REVIEW:', review)\n",
        "    print('Actual Sentiment:', sentiment)\n",
        "    print('Predicted Sentiment polarity:', afn.score(review))\n",
        "    print('-'*60)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REVIEW: I am a firm believer and user of Maple Syrup over Corn Syrup or white sugar.  It is, simply, a healthier sugar - if sugar can be healthy.<br /><br />I'm sure the syrup will meet all of my taste expectations.<br /><br />The syrup was securely packaged and arrived sooner than I expected.  No complaints.\n",
            "Actual Sentiment: 1\n",
            "Predicted Sentiment polarity: -1.0\n",
            "------------------------------------------------------------\n",
            "REVIEW: I really wanted to find a rice cereal that had more nutritional value than just plain white rice and I am so happy I found this product. I started my baby on this at 5 months and he loves it. I mix it with Breast Milk and once he starts veggies and fruit I will add that in as well. This does not constipate him either and I love that it is Organic and enriched with Probiotcs and DHA.<br />If you are looking for a great first food for your baby with quality ingredients this is it and you can't beat the price for organic.\n",
            "Actual Sentiment: 1\n",
            "Predicted Sentiment polarity: 14.0\n",
            "------------------------------------------------------------\n",
            "REVIEW: We drink a lot of green tea and TAZO is one of our favorites. It's a good deal when I buy it on amazon prime. 6 boxes of 20 for $4\n",
            "Actual Sentiment: 1\n",
            "Predicted Sentiment polarity: 5.0\n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT4OMOtnDRun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_polarity = [afn.score(review) for review in test_reviews]\n",
        "predicted_sentiments = [1 if score >= 1.0 else 0 for score in sentiment_polarity]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSa5CCPqIPZn",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md0eh6xrDRqy",
        "colab_type": "code",
        "outputId": "c605699b-a183-49d0-cac1-d26b87b78c2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, predicted_sentiments))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, predicted_sentiments), index=labels, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.40      0.46      1201\n",
            "           1       0.89      0.93      0.91      6224\n",
            "\n",
            "    accuracy                           0.85      7425\n",
            "   macro avg       0.72      0.67      0.69      7425\n",
            "weighted avg       0.83      0.85      0.84      7425\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>484</td>\n",
              "      <td>717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>408</td>\n",
              "      <td>5816</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative       484       717\n",
              "positive       408      5816"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq0h84jwIvoK",
        "colab_type": "text"
      },
      "source": [
        "### Using VADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4TZnwUhDRpM",
        "colab_type": "code",
        "outputId": "2bbebd00-9561-4947-d5c3-803ef23dcfbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "import contractions\n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.strip()\n",
        "    doc = contractions.fix(doc)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "\n",
        "norm_corpus = normalize_corpus(test_reviews)\n",
        "len(norm_corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7425"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZyxcdWjDRly",
        "colab_type": "code",
        "outputId": "d744dd61-f093-4566-cbc6-07db70f1a6c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "def analyze_sentiment_vader_lexicon(review, \n",
        "                                    threshold=0.1,\n",
        "                                    verbose=False):    \n",
        "    # analyze the sentiment for review\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    scores = analyzer.polarity_scores(review)\n",
        "    # get aggregate scores and final sentiment\n",
        "    agg_score = scores['compound']\n",
        "    final_sentiment = 1 if agg_score >= threshold\\\n",
        "                                   else 0\n",
        "    if verbose:\n",
        "        # display detailed sentiment statistics\n",
        "        positive = str(round(scores['pos'], 2)*100)+'%'\n",
        "        final = round(agg_score, 2)\n",
        "        negative = str(round(scores['neg'], 2)*100)+'%'\n",
        "        neutral = str(round(scores['neu'], 2)*100)+'%'\n",
        "        sentiment_frame = pd.DataFrame([[final_sentiment, final, positive,\n",
        "                                        negative, neutral]],\n",
        "                                        columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
        "                                                                      ['Predicted Sentiment', 'Polarity Score',\n",
        "                                                                       'Positive', 'Negative', 'Neutral']], \n",
        "                                                              codes=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
        "        print(sentiment_frame)\n",
        "    \n",
        "    return final_sentiment"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5GeGUREKYZV",
        "colab_type": "text"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs8mBbPlDRjD",
        "colab_type": "code",
        "outputId": "e4fc4ddf-cd4a-484a-dc68-762742f6a4f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "for review, sentiment in zip(norm_corpus[sample_review_ids], test_sentiments[sample_review_ids]):\n",
        "    print('REVIEW:', review)\n",
        "    print('Actual Sentiment:', sentiment)\n",
        "    pred = analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=True)    \n",
        "    print('-'*60)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REVIEW: I am a firm believer and user of Maple Syrup over Corn Syrup or white sugar  It is simply a healthier sugar  if sugar can be healthybr br I am sure the syrup will meet all of my taste expectationsbr br The syrup was securely packaged and arrived sooner than I expected  No complaints\n",
            "Actual Sentiment: 1\n",
            "     SENTIMENT STATS:                                         \n",
            "  Predicted Sentiment Polarity Score Positive Negative Neutral\n",
            "0                   0          -0.05     9.0%     9.0%   82.0%\n",
            "------------------------------------------------------------\n",
            "REVIEW: I really wanted to find a rice cereal that had more nutritional value than just plain white rice and I am so happy I found this product I started my baby on this at 5 months and he loves it I mix it with Breast Milk and once he starts veggies and fruit I will add that in as well This does not constipate him either and I love that it is Organic and enriched with Probiotcs and DHAbr If you are looking for a great first food for your baby with quality ingredients this is it and you can not beat the price for organic\n",
            "Actual Sentiment: 1\n",
            "     SENTIMENT STATS:                                         \n",
            "  Predicted Sentiment Polarity Score Positive Negative Neutral\n",
            "0                   1           0.97    19.0%     0.0%   81.0%\n",
            "------------------------------------------------------------\n",
            "REVIEW: We drink a lot of green tea and TAZO is one of our favorites Its a good deal when I buy it on amazon prime 6 boxes of 20 for 4\n",
            "Actual Sentiment: 1\n",
            "     SENTIMENT STATS:                                         \n",
            "  Predicted Sentiment Polarity Score Positive Negative Neutral\n",
            "0                   1           0.75    24.0%     0.0%   76.0%\n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL-H3u0eDRXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_sentiments = [analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=False) for review in test_reviews]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w4JSKfFK7-5",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGI4iThuLA_4",
        "colab_type": "code",
        "outputId": "6e395c45-c496-4a75-e6d9-d77aa449dee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, predicted_sentiments))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, predicted_sentiments), index=labels, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.57      0.54      1201\n",
            "           1       0.92      0.89      0.90      6224\n",
            "\n",
            "    accuracy                           0.84      7425\n",
            "   macro avg       0.71      0.73      0.72      7425\n",
            "weighted avg       0.85      0.84      0.84      7425\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>690</td>\n",
              "      <td>511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>684</td>\n",
              "      <td>5540</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative       690       511\n",
              "positive       684      5540"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I7K5PxwNFEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}